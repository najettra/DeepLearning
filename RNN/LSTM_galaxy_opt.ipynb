{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import talos as ta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam, Nadam, RMSprop\n",
    "from keras.callbacks import EarlyStopping,TensorBoard,ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.activations import relu, elu, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('galaxyS8_DE_21.240564489661963%.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6472 entries, 0 to 6471\n",
      "Data columns (total 9 columns):\n",
      "timestamp      6472 non-null object\n",
      "category       6472 non-null object\n",
      "cleanName      6472 non-null object\n",
      "country        6472 non-null object\n",
      "market_name    6472 non-null object\n",
      "memory         6472 non-null object\n",
      "name           6472 non-null object\n",
      "price          6472 non-null float64\n",
      "url            6472 non-null object\n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 455.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>category</th>\n",
       "      <th>cleanName</th>\n",
       "      <th>country</th>\n",
       "      <th>market_name</th>\n",
       "      <th>memory</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-21 08:41:08</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Galaxy S8 Plus</td>\n",
       "      <td>DE</td>\n",
       "      <td>Galaxy S8 Plus</td>\n",
       "      <td>64GB</td>\n",
       "      <td>Samsung Galaxy S8+ Smartphone (6,2 Zoll (15,8 ...</td>\n",
       "      <td>659.9</td>\n",
       "      <td>https://www.amazon.de/Samsung-Smartphone-Touch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-20 18:31:39</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Galaxy S8 Plus</td>\n",
       "      <td>DE</td>\n",
       "      <td>Galaxy S8 Plus</td>\n",
       "      <td>64GB</td>\n",
       "      <td>Samsung Galaxy S8+ Smartphone (6,2 Zoll (15,8 ...</td>\n",
       "      <td>642.0</td>\n",
       "      <td>https://www.amazon.de/Samsung-Smartphone-Touch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-20 18:54:35</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Galaxy S8 Plus</td>\n",
       "      <td>DE</td>\n",
       "      <td>Galaxy S8 Plus</td>\n",
       "      <td>64GB</td>\n",
       "      <td>Samsung Galaxy S8+ Smartphone (6,2 Zoll (15,8 ...</td>\n",
       "      <td>659.9</td>\n",
       "      <td>https://www.amazon.de/Samsung-Smartphone-Touch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-20 18:16:22</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Galaxy S8 Plus</td>\n",
       "      <td>DE</td>\n",
       "      <td>Galaxy S8 Plus</td>\n",
       "      <td>64GB</td>\n",
       "      <td>Samsung Galaxy S8+ Smartphone (6,2 Zoll (15,8 ...</td>\n",
       "      <td>642.0</td>\n",
       "      <td>https://www.amazon.de/Samsung-Smartphone-Touch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-20 14:44:18</td>\n",
       "      <td>Phone</td>\n",
       "      <td>Galaxy S8 Plus</td>\n",
       "      <td>DE</td>\n",
       "      <td>Galaxy S8 Plus</td>\n",
       "      <td>64GB</td>\n",
       "      <td>Samsung Galaxy S8+ Smartphone (6,2 Zoll (15,8 ...</td>\n",
       "      <td>585.0</td>\n",
       "      <td>https://www.amazon.de/Samsung-Smartphone-Touch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp category       cleanName country     market_name  \\\n",
       "0  2017-12-21 08:41:08    Phone  Galaxy S8 Plus      DE  Galaxy S8 Plus   \n",
       "1  2017-12-20 18:31:39    Phone  Galaxy S8 Plus      DE  Galaxy S8 Plus   \n",
       "2  2017-12-20 18:54:35    Phone  Galaxy S8 Plus      DE  Galaxy S8 Plus   \n",
       "3  2017-12-20 18:16:22    Phone  Galaxy S8 Plus      DE  Galaxy S8 Plus   \n",
       "4  2017-12-20 14:44:18    Phone  Galaxy S8 Plus      DE  Galaxy S8 Plus   \n",
       "\n",
       "  memory                                               name  price  \\\n",
       "0   64GB  Samsung Galaxy S8+ Smartphone (6,2 Zoll (15,8 ...  659.9   \n",
       "1   64GB  Samsung Galaxy S8+ Smartphone (6,2 Zoll (15,8 ...  642.0   \n",
       "2   64GB  Samsung Galaxy S8+ Smartphone (6,2 Zoll (15,8 ...  659.9   \n",
       "3   64GB  Samsung Galaxy S8+ Smartphone (6,2 Zoll (15,8 ...  642.0   \n",
       "4   64GB  Samsung Galaxy S8+ Smartphone (6,2 Zoll (15,8 ...  585.0   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.amazon.de/Samsung-Smartphone-Touch...  \n",
       "1  https://www.amazon.de/Samsung-Smartphone-Touch...  \n",
       "2  https://www.amazon.de/Samsung-Smartphone-Touch...  \n",
       "3  https://www.amazon.de/Samsung-Smartphone-Touch...  \n",
       "4  https://www.amazon.de/Samsung-Smartphone-Touch...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data[['timestamp', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-21 08:41:08</td>\n",
       "      <td>659.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-20 18:31:39</td>\n",
       "      <td>642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-20 18:54:35</td>\n",
       "      <td>659.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-20 18:16:22</td>\n",
       "      <td>642.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-20 14:44:18</td>\n",
       "      <td>585.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  price\n",
       "0  2017-12-21 08:41:08  659.9\n",
       "1  2017-12-20 18:31:39  642.0\n",
       "2  2017-12-20 18:54:35  659.9\n",
       "3  2017-12-20 18:16:22  642.0\n",
       "4  2017-12-20 14:44:18  585.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timestamp    0\n",
       "price        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertToTimeSeries(DF,window):\n",
    "    DF.index= pd.to_datetime(DF['timestamp'])\n",
    "    DF=DF.resample(\"6H\").mean()\n",
    "    DF.sort_index(inplace=True)\n",
    "    \n",
    "    DF['t'] = [x for x in DF['price']]\n",
    "    DF['t-1'] = DF['t'].shift(1)\n",
    "    for i in range(1,window):\n",
    "        DF['t-'+str(i+1)] = DF['t-'+str(i)].shift(1)\n",
    "        \n",
    "    DF.dropna(inplace=True)\n",
    "    DF.drop(['t'], 1 , inplace=True)\n",
    "    \n",
    "    return(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_samsg = ConvertToTimeSeries(dataset,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>t-1</th>\n",
       "      <th>t-2</th>\n",
       "      <th>t-3</th>\n",
       "      <th>t-4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-12-10 00:00:00</th>\n",
       "      <td>660.663226</td>\n",
       "      <td>649.769091</td>\n",
       "      <td>675.176364</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>756.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-10 06:00:00</th>\n",
       "      <td>653.350000</td>\n",
       "      <td>660.663226</td>\n",
       "      <td>649.769091</td>\n",
       "      <td>675.176364</td>\n",
       "      <td>649.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-10 12:00:00</th>\n",
       "      <td>685.348400</td>\n",
       "      <td>653.350000</td>\n",
       "      <td>660.663226</td>\n",
       "      <td>649.769091</td>\n",
       "      <td>675.176364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-10 18:00:00</th>\n",
       "      <td>656.434091</td>\n",
       "      <td>685.348400</td>\n",
       "      <td>653.350000</td>\n",
       "      <td>660.663226</td>\n",
       "      <td>649.769091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-11 00:00:00</th>\n",
       "      <td>660.020000</td>\n",
       "      <td>656.434091</td>\n",
       "      <td>685.348400</td>\n",
       "      <td>653.350000</td>\n",
       "      <td>660.663226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          price         t-1         t-2         t-3  \\\n",
       "timestamp                                                             \n",
       "2017-12-10 00:00:00  660.663226  649.769091  675.176364  649.000000   \n",
       "2017-12-10 06:00:00  653.350000  660.663226  649.769091  675.176364   \n",
       "2017-12-10 12:00:00  685.348400  653.350000  660.663226  649.769091   \n",
       "2017-12-10 18:00:00  656.434091  685.348400  653.350000  660.663226   \n",
       "2017-12-11 00:00:00  660.020000  656.434091  685.348400  653.350000   \n",
       "\n",
       "                            t-4  \n",
       "timestamp                        \n",
       "2017-12-10 00:00:00  756.590000  \n",
       "2017-12-10 06:00:00  649.000000  \n",
       "2017-12-10 12:00:00  675.176364  \n",
       "2017-12-10 18:00:00  649.769091  \n",
       "2017-12-11 00:00:00  660.663226  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_samsg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajustement du module d'apprentissage\n",
    "def fitLSTM(DF,window):\n",
    "    df= ConvertToTimeSeries(DF,window)\n",
    "    \n",
    "    X = np.array(df.drop(['price'],1))\n",
    "    \n",
    "    y = np.array(df['price'])\n",
    "    X = scale(X)\n",
    "    \n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = fitLSTM(dataset,4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((81, 4), (21, 4))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajustement des dimensions des Inputs\n",
    "X_train=np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "X_test=np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "input_size=X_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((81, 4, 1), (21, 4, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 4, 300)            362400    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 1,083,901\n",
      "Trainable params: 1,083,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Creation du model LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(300,return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dense(1))\n",
    "adam = Adam(lr=0.005)\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='GALAXY'\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=os.path.join(model_name + '.hdf5'),\n",
    "    monitor='val_loss', verbose=1, save_best_only=True, mode='min', period=1)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, mode='min', verbose=1)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./Graph/'+model_name,\n",
    "                          histogram_freq=1,\n",
    "                          write_grads=True,\n",
    "                          # batch_size=batch_size,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp=ModelCheckpoint('galaxy_lstm.hdf5',monitor='val_loss',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81 samples, validate on 21 samples\n",
      "Epoch 1/100\n",
      "81/81 [==============================] - 8s 99ms/step - loss: 475281.4236 - val_loss: 399331.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 399331.00000, saving model to galaxy_lstm.hdf5\n",
      "Epoch 2/100\n",
      "81/81 [==============================] - 4s 53ms/step - loss: 426556.4541 - val_loss: 365343.1042\n",
      "\n",
      "Epoch 00002: val_loss improved from 399331.00000 to 365343.10417, saving model to galaxy_lstm.hdf5\n",
      "Epoch 3/100\n",
      "81/81 [==============================] - 5s 61ms/step - loss: 393078.9061 - val_loss: 335321.6711\n",
      "\n",
      "Epoch 00003: val_loss improved from 365343.10417 to 335321.67113, saving model to galaxy_lstm.hdf5\n",
      "Epoch 4/100\n",
      "81/81 [==============================] - 6s 76ms/step - loss: 362762.4286 - val_loss: 307900.8683\n",
      "\n",
      "Epoch 00004: val_loss improved from 335321.67113 to 307900.86830, saving model to galaxy_lstm.hdf5\n",
      "Epoch 5/100\n",
      "81/81 [==============================] - 7s 87ms/step - loss: 334812.4497 - val_loss: 282526.3467\n",
      "\n",
      "Epoch 00005: val_loss improved from 307900.86830 to 282526.34673, saving model to galaxy_lstm.hdf5\n",
      "Epoch 6/100\n",
      "81/81 [==============================] - 7s 82ms/step - loss: 308782.2477 - val_loss: 259025.9963\n",
      "\n",
      "Epoch 00006: val_loss improved from 282526.34673 to 259025.99628, saving model to galaxy_lstm.hdf5\n",
      "Epoch 7/100\n",
      "81/81 [==============================] - 5s 62ms/step - loss: 284501.6161 - val_loss: 236996.2150\n",
      "\n",
      "Epoch 00007: val_loss improved from 259025.99628 to 236996.21503, saving model to galaxy_lstm.hdf5\n",
      "Epoch 8/100\n",
      "81/81 [==============================] - 6s 75ms/step - loss: 261762.7478 - val_loss: 216433.9040\n",
      "\n",
      "Epoch 00008: val_loss improved from 236996.21503 to 216433.90402, saving model to galaxy_lstm.hdf5\n",
      "Epoch 9/100\n",
      "81/81 [==============================] - 6s 70ms/step - loss: 240461.9818 - val_loss: 197383.2188\n",
      "\n",
      "Epoch 00009: val_loss improved from 216433.90402 to 197383.21875, saving model to galaxy_lstm.hdf5\n",
      "Epoch 10/100\n",
      "81/81 [==============================] - 6s 75ms/step - loss: 220566.6901 - val_loss: 179459.0938\n",
      "\n",
      "Epoch 00010: val_loss improved from 197383.21875 to 179459.09375, saving model to galaxy_lstm.hdf5\n",
      "Epoch 11/100\n",
      "81/81 [==============================] - 8s 100ms/step - loss: 201936.1354 - val_loss: 162818.4200\n",
      "\n",
      "Epoch 00011: val_loss improved from 179459.09375 to 162818.42001, saving model to galaxy_lstm.hdf5\n",
      "Epoch 12/100\n",
      "81/81 [==============================] - 6s 77ms/step - loss: 184517.1338 - val_loss: 147344.2422\n",
      "\n",
      "Epoch 00012: val_loss improved from 162818.42001 to 147344.24219, saving model to galaxy_lstm.hdf5\n",
      "Epoch 13/100\n",
      "81/81 [==============================] - 7s 83ms/step - loss: 168272.4329 - val_loss: 132935.3445\n",
      "\n",
      "Epoch 00013: val_loss improved from 147344.24219 to 132935.34449, saving model to galaxy_lstm.hdf5\n",
      "Epoch 14/100\n",
      "81/81 [==============================] - 7s 86ms/step - loss: 153130.0191 - val_loss: 119636.5606\n",
      "\n",
      "Epoch 00014: val_loss improved from 132935.34449 to 119636.56064, saving model to galaxy_lstm.hdf5\n",
      "Epoch 15/100\n",
      "81/81 [==============================] - 7s 89ms/step - loss: 139059.0376 - val_loss: 107354.0272\n",
      "\n",
      "Epoch 00015: val_loss improved from 119636.56064 to 107354.02716, saving model to galaxy_lstm.hdf5\n",
      "Epoch 16/100\n",
      "81/81 [==============================] - 8s 99ms/step - loss: 125997.9067 - val_loss: 95981.7768\n",
      "\n",
      "Epoch 00016: val_loss improved from 107354.02716 to 95981.77679, saving model to galaxy_lstm.hdf5\n",
      "Epoch 17/100\n",
      "81/81 [==============================] - 8s 100ms/step - loss: 113879.1851 - val_loss: 85522.7355\n",
      "\n",
      "Epoch 00017: val_loss improved from 95981.77679 to 85522.73549, saving model to galaxy_lstm.hdf5\n",
      "Epoch 18/100\n",
      "81/81 [==============================] - 8s 99ms/step - loss: 102683.7338 - val_loss: 75893.9116\n",
      "\n",
      "Epoch 00018: val_loss improved from 85522.73549 to 75893.91164, saving model to galaxy_lstm.hdf5\n",
      "Epoch 19/100\n",
      "81/81 [==============================] - 7s 83ms/step - loss: 92358.2222 - val_loss: 67080.7165\n",
      "\n",
      "Epoch 00019: val_loss improved from 75893.91164 to 67080.71652, saving model to galaxy_lstm.hdf5\n",
      "Epoch 20/100\n",
      "81/81 [==============================] - 7s 83ms/step - loss: 82866.9708 - val_loss: 59039.1085\n",
      "\n",
      "Epoch 00020: val_loss improved from 67080.71652 to 59039.10854, saving model to galaxy_lstm.hdf5\n",
      "Epoch 21/100\n",
      "81/81 [==============================] - 6s 73ms/step - loss: 74149.2230 - val_loss: 51810.0095\n",
      "\n",
      "Epoch 00021: val_loss improved from 59039.10854 to 51810.00949, saving model to galaxy_lstm.hdf5\n",
      "Epoch 22/100\n",
      "81/81 [==============================] - 7s 81ms/step - loss: 66189.4422 - val_loss: 45201.7733\n",
      "\n",
      "Epoch 00022: val_loss improved from 51810.00949 to 45201.77325, saving model to galaxy_lstm.hdf5\n",
      "Epoch 23/100\n",
      "81/81 [==============================] - 6s 78ms/step - loss: 58914.8896 - val_loss: 39270.4526\n",
      "\n",
      "Epoch 00023: val_loss improved from 45201.77325 to 39270.45257, saving model to galaxy_lstm.hdf5\n",
      "Epoch 24/100\n",
      "81/81 [==============================] - 6s 76ms/step - loss: 52315.2483 - val_loss: 33910.7432\n",
      "\n",
      "Epoch 00024: val_loss improved from 39270.45257 to 33910.74316, saving model to galaxy_lstm.hdf5\n",
      "Epoch 25/100\n",
      "81/81 [==============================] - 7s 87ms/step - loss: 46316.3363 - val_loss: 29167.2569\n",
      "\n",
      "Epoch 00025: val_loss improved from 33910.74316 to 29167.25688, saving model to galaxy_lstm.hdf5\n",
      "Epoch 26/100\n",
      "81/81 [==============================] - 6s 75ms/step - loss: 40887.4942 - val_loss: 24973.5057\n",
      "\n",
      "Epoch 00026: val_loss improved from 29167.25688 to 24973.50567, saving model to galaxy_lstm.hdf5\n",
      "Epoch 27/100\n",
      "81/81 [==============================] - 6s 79ms/step - loss: 36010.0800 - val_loss: 21182.9784\n",
      "\n",
      "Epoch 00027: val_loss improved from 24973.50567 to 21182.97842, saving model to galaxy_lstm.hdf5\n",
      "Epoch 28/100\n",
      "81/81 [==============================] - 6s 79ms/step - loss: 31635.0651 - val_loss: 17849.3766\n",
      "\n",
      "Epoch 00028: val_loss improved from 21182.97842 to 17849.37663, saving model to galaxy_lstm.hdf5\n",
      "Epoch 29/100\n",
      "81/81 [==============================] - 8s 95ms/step - loss: 27720.1034 - val_loss: 14957.5990\n",
      "\n",
      "Epoch 00029: val_loss improved from 17849.37663 to 14957.59904, saving model to galaxy_lstm.hdf5\n",
      "Epoch 30/100\n",
      "81/81 [==============================] - 7s 90ms/step - loss: 24246.4903 - val_loss: 12449.3499\n",
      "\n",
      "Epoch 00030: val_loss improved from 14957.59904 to 12449.34988, saving model to galaxy_lstm.hdf5\n",
      "Epoch 31/100\n",
      "81/81 [==============================] - 8s 96ms/step - loss: 21162.8719 - val_loss: 10324.6322\n",
      "\n",
      "Epoch 00031: val_loss improved from 12449.34988 to 10324.63220, saving model to galaxy_lstm.hdf5\n",
      "Epoch 32/100\n",
      "81/81 [==============================] - 8s 98ms/step - loss: 18454.8771 - val_loss: 8468.2865\n",
      "\n",
      "Epoch 00032: val_loss improved from 10324.63220 to 8468.28647, saving model to galaxy_lstm.hdf5\n",
      "Epoch 33/100\n",
      "81/81 [==============================] - 7s 92ms/step - loss: 16075.6901 - val_loss: 6886.8497\n",
      "\n",
      "Epoch 00033: val_loss improved from 8468.28647 to 6886.84969, saving model to galaxy_lstm.hdf5\n",
      "Epoch 34/100\n",
      "81/81 [==============================] - 8s 95ms/step - loss: 14011.0078 - val_loss: 5563.5151\n",
      "\n",
      "Epoch 00034: val_loss improved from 6886.84969 to 5563.51515, saving model to galaxy_lstm.hdf5\n",
      "Epoch 35/100\n",
      "81/81 [==============================] - 7s 89ms/step - loss: 12210.0296 - val_loss: 4528.7149\n",
      "\n",
      "Epoch 00035: val_loss improved from 5563.51515 to 4528.71486, saving model to galaxy_lstm.hdf5\n",
      "Epoch 36/100\n",
      "81/81 [==============================] - 7s 89ms/step - loss: 10675.0202 - val_loss: 3652.1781\n",
      "\n",
      "Epoch 00036: val_loss improved from 4528.71486 to 3652.17812, saving model to galaxy_lstm.hdf5\n",
      "Epoch 37/100\n",
      "81/81 [==============================] - 7s 92ms/step - loss: 9345.0411 - val_loss: 2983.6252\n",
      "\n",
      "Epoch 00037: val_loss improved from 3652.17812 to 2983.62521, saving model to galaxy_lstm.hdf5\n",
      "Epoch 38/100\n",
      "81/81 [==============================] - 8s 94ms/step - loss: 8238.7420 - val_loss: 2413.1914\n",
      "\n",
      "Epoch 00038: val_loss improved from 2983.62521 to 2413.19136, saving model to galaxy_lstm.hdf5\n",
      "Epoch 39/100\n",
      "81/81 [==============================] - 9s 109ms/step - loss: 7283.2812 - val_loss: 2020.6450\n",
      "\n",
      "Epoch 00039: val_loss improved from 2413.19136 to 2020.64498, saving model to galaxy_lstm.hdf5\n",
      "Epoch 40/100\n",
      "81/81 [==============================] - 7s 92ms/step - loss: 6495.6912 - val_loss: 1725.6859\n",
      "\n",
      "Epoch 00040: val_loss improved from 2020.64498 to 1725.68591, saving model to galaxy_lstm.hdf5\n",
      "Epoch 41/100\n",
      "81/81 [==============================] - 7s 83ms/step - loss: 5837.2522 - val_loss: 1531.7030\n",
      "\n",
      "Epoch 00041: val_loss improved from 1725.68591 to 1531.70297, saving model to galaxy_lstm.hdf5\n",
      "Epoch 42/100\n",
      "81/81 [==============================] - 7s 90ms/step - loss: 5295.0027 - val_loss: 1408.5584\n",
      "\n",
      "Epoch 00042: val_loss improved from 1531.70297 to 1408.55838, saving model to galaxy_lstm.hdf5\n",
      "Epoch 43/100\n",
      "81/81 [==============================] - 7s 90ms/step - loss: 4850.5825 - val_loss: 1349.9314\n",
      "\n",
      "Epoch 00043: val_loss improved from 1408.55838 to 1349.93140, saving model to galaxy_lstm.hdf5\n",
      "Epoch 44/100\n",
      "81/81 [==============================] - 7s 90ms/step - loss: 4503.7012 - val_loss: 1336.6168\n",
      "\n",
      "Epoch 00044: val_loss improved from 1349.93140 to 1336.61682, saving model to galaxy_lstm.hdf5\n",
      "Epoch 45/100\n",
      "81/81 [==============================] - 7s 88ms/step - loss: 4205.6111 - val_loss: 1359.4162\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1336.61682\n",
      "Epoch 46/100\n",
      "81/81 [==============================] - 9s 107ms/step - loss: 3981.1080 - val_loss: 1408.1558\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1336.61682\n",
      "Epoch 47/100\n",
      "81/81 [==============================] - 8s 102ms/step - loss: 3808.7892 - val_loss: 1485.0933\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1336.61682\n",
      "Epoch 48/100\n",
      "81/81 [==============================] - 8s 103ms/step - loss: 3665.2425 - val_loss: 1561.4979\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1336.61682\n",
      "Epoch 49/100\n",
      "81/81 [==============================] - 9s 113ms/step - loss: 3562.6964 - val_loss: 1651.7693\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1336.61682\n",
      "Epoch 50/100\n",
      "81/81 [==============================] - 9s 111ms/step - loss: 3482.1608 - val_loss: 1751.9893\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1336.61682\n",
      "Epoch 51/100\n",
      "81/81 [==============================] - 9s 111ms/step - loss: 3417.1253 - val_loss: 1829.8177\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1336.61682\n",
      "Epoch 52/100\n",
      "81/81 [==============================] - 9s 107ms/step - loss: 3368.8716 - val_loss: 1919.6272\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1336.61682\n",
      "Epoch 53/100\n",
      "81/81 [==============================] - 9s 106ms/step - loss: 3343.3842 - val_loss: 2008.9958\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1336.61682\n",
      "Epoch 54/100\n",
      "81/81 [==============================] - 9s 105ms/step - loss: 3315.3096 - val_loss: 2078.4013\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1336.61682\n",
      "Epoch 55/100\n",
      "81/81 [==============================] - 9s 108ms/step - loss: 3299.6709 - val_loss: 2134.9667\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1336.61682\n",
      "Epoch 56/100\n",
      "81/81 [==============================] - 9s 110ms/step - loss: 3286.1862 - val_loss: 2172.2900\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1336.61682\n",
      "Epoch 57/100\n",
      "81/81 [==============================] - 9s 111ms/step - loss: 3281.5019 - val_loss: 2235.9606\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1336.61682\n",
      "Epoch 58/100\n",
      "81/81 [==============================] - 9s 106ms/step - loss: 3270.8679 - val_loss: 2291.2069\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1336.61682\n",
      "Epoch 59/100\n",
      "81/81 [==============================] - 9s 108ms/step - loss: 3268.6077 - val_loss: 2299.0111\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1336.61682\n",
      "Epoch 60/100\n",
      "81/81 [==============================] - 9s 108ms/step - loss: 3264.8465 - val_loss: 2354.5802\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1336.61682\n",
      "Epoch 61/100\n",
      "81/81 [==============================] - 8s 94ms/step - loss: 3262.1292 - val_loss: 2357.7559\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1336.61682\n",
      "Epoch 62/100\n",
      "81/81 [==============================] - 8s 99ms/step - loss: 3265.8235 - val_loss: 2401.7831\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1336.61682\n",
      "Epoch 63/100\n",
      "81/81 [==============================] - 9s 105ms/step - loss: 3262.3505 - val_loss: 2389.7298\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1336.61682\n",
      "Epoch 64/100\n",
      "81/81 [==============================] - 9s 108ms/step - loss: 3261.1768 - val_loss: 2428.0320\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1336.61682\n",
      "Epoch 65/100\n",
      "81/81 [==============================] - 9s 108ms/step - loss: 3261.3218 - val_loss: 2407.6176\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1336.61682\n",
      "Epoch 66/100\n",
      "81/81 [==============================] - 10s 119ms/step - loss: 3265.6688 - val_loss: 2474.9694\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1336.61682\n",
      "Epoch 67/100\n",
      "81/81 [==============================] - 9s 106ms/step - loss: 3262.8241 - val_loss: 2422.3429\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1336.61682\n",
      "Epoch 68/100\n",
      "81/81 [==============================] - 9s 106ms/step - loss: 3264.0755 - val_loss: 2415.7905\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1336.61682\n",
      "Epoch 69/100\n",
      "81/81 [==============================] - 9s 107ms/step - loss: 3263.7410 - val_loss: 2448.3946\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1336.61682\n",
      "Epoch 70/100\n",
      "81/81 [==============================] - 9s 116ms/step - loss: 3265.6978 - val_loss: 2473.0244\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1336.61682\n",
      "Epoch 71/100\n",
      "81/81 [==============================] - 9s 114ms/step - loss: 3259.9533 - val_loss: 2479.5969\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1336.61682\n",
      "Epoch 72/100\n",
      "81/81 [==============================] - 9s 110ms/step - loss: 3260.9157 - val_loss: 2468.2544\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1336.61682\n",
      "Epoch 73/100\n",
      "81/81 [==============================] - 9s 110ms/step - loss: 3272.4346 - val_loss: 2450.8562\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1336.61682\n",
      "Epoch 74/100\n",
      "81/81 [==============================] - 9s 112ms/step - loss: 3259.3088 - val_loss: 2503.7269\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1336.61682\n",
      "Epoch 75/100\n",
      "81/81 [==============================] - 9s 116ms/step - loss: 3259.9313 - val_loss: 2468.2338\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1336.61682\n",
      "Epoch 76/100\n",
      "81/81 [==============================] - 9s 111ms/step - loss: 3245.5339 - val_loss: 2453.6159\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1336.61682\n",
      "Epoch 77/100\n",
      "81/81 [==============================] - 10s 128ms/step - loss: 3242.6630 - val_loss: 2448.1048\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1336.61682\n",
      "Epoch 78/100\n",
      "81/81 [==============================] - 8s 94ms/step - loss: 3241.4125 - val_loss: 2420.4105\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1336.61682\n",
      "Epoch 79/100\n",
      "81/81 [==============================] - 8s 97ms/step - loss: 3234.2986 - val_loss: 2399.2113\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1336.61682\n",
      "Epoch 80/100\n",
      "81/81 [==============================] - 8s 99ms/step - loss: 3222.5721 - val_loss: 2455.8943\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1336.61682\n",
      "Epoch 81/100\n",
      "81/81 [==============================] - 8s 103ms/step - loss: 3208.0498 - val_loss: 2401.2248\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1336.61682\n",
      "Epoch 82/100\n",
      "81/81 [==============================] - 8s 101ms/step - loss: 3235.3491 - val_loss: 2188.4563\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1336.61682\n",
      "Epoch 83/100\n",
      "81/81 [==============================] - 8s 99ms/step - loss: 3180.6857 - val_loss: 2325.6494\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1336.61682\n",
      "Epoch 84/100\n",
      "81/81 [==============================] - 8s 101ms/step - loss: 3033.5748 - val_loss: 1802.6580\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1336.61682\n",
      "Epoch 85/100\n",
      "81/81 [==============================] - 8s 100ms/step - loss: 2893.6547 - val_loss: 1566.7128\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1336.61682\n",
      "Epoch 86/100\n",
      "81/81 [==============================] - 8s 103ms/step - loss: 2900.6460 - val_loss: 1775.7718\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1336.61682\n",
      "Epoch 87/100\n",
      "81/81 [==============================] - 8s 101ms/step - loss: 2885.6326 - val_loss: 1710.0068\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1336.61682\n",
      "Epoch 88/100\n",
      "81/81 [==============================] - 7s 88ms/step - loss: 2816.4903 - val_loss: 1722.7075\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1336.61682\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 7s 82ms/step - loss: 2810.5842 - val_loss: 2111.6236\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1336.61682\n",
      "Epoch 90/100\n",
      "81/81 [==============================] - 5s 68ms/step - loss: 2809.9595 - val_loss: 1714.5562\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1336.61682\n",
      "Epoch 91/100\n",
      "81/81 [==============================] - 6s 72ms/step - loss: 2728.6302 - val_loss: 1498.7993\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1336.61682\n",
      "Epoch 92/100\n",
      "81/81 [==============================] - 6s 75ms/step - loss: 2673.6616 - val_loss: 2304.9378\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1336.61682\n",
      "Epoch 93/100\n",
      "81/81 [==============================] - 6s 72ms/step - loss: 2744.8616 - val_loss: 1731.8344\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1336.61682\n",
      "Epoch 94/100\n",
      "81/81 [==============================] - 6s 76ms/step - loss: 2734.1204 - val_loss: 1542.9518\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1336.61682\n",
      "Epoch 95/100\n",
      "81/81 [==============================] - 7s 91ms/step - loss: 2662.2127 - val_loss: 1698.7306\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1336.61682\n",
      "Epoch 96/100\n",
      "81/81 [==============================] - 8s 103ms/step - loss: 2586.3945 - val_loss: 1563.8858\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1336.61682\n",
      "Epoch 97/100\n",
      "81/81 [==============================] - 8s 95ms/step - loss: 2576.4245 - val_loss: 1674.7524\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1336.61682\n",
      "Epoch 98/100\n",
      "81/81 [==============================] - 8s 95ms/step - loss: 2451.5708 - val_loss: 1750.9201\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1336.61682\n",
      "Epoch 99/100\n",
      "81/81 [==============================] - 7s 92ms/step - loss: 2440.2167 - val_loss: 1791.6030\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1336.61682\n",
      "Epoch 100/100\n",
      "81/81 [==============================] - 8s 95ms/step - loss: 2475.4021 - val_loss: 1789.2479\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1336.61682\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=1, epochs=100, validation_data=(X_test, y_test), callbacks=[cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "'''plt.figure(figsize=(8, 6), dpi=100)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(os.path.join('model accuracy 3.png'))\n",
    "plt.show()'''\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(8, 6), dpi=100)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(os.path.join('model loss 3.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def RMSE(y_test,Y_new):\n",
    "    return np.sqrt(sklearn.metrics.mean_squared_error(y_test-Y_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.29948974488621\n"
     ]
    }
   ],
   "source": [
    "Y_new = model.predict(X_test)\n",
    "rmse = np.sqrt(sklearn.metrics.mean_squared_error(y_test,Y_new))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('galaxy_lstm.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.55974921867143\n"
     ]
    }
   ],
   "source": [
    "Y_new = best_model.predict(X_test)\n",
    "rmse = np.sqrt(sklearn.metrics.mean_squared_error(y_test,Y_new))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def galaxy_model(X_train, y_train, X_test, y_test, params):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['LSTM1'], return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(params['LSTM2']))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=params['optimizer'](),\n",
    "                  loss='mean_squared_error', metrics=['acc'])\n",
    "    \n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=params['epochs'])\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    epochs=params['epochs'],\n",
    "                    verbose=0,\n",
    "                    validation_data=[X_test, y_test],\n",
    "                    callbacks=[checkpointer, early_stopping_callback])\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {'lr': [2, 10, 30],\n",
    "     'LSTM1':[200, 300, 400],\n",
    "     'LSTM2':[200, 300, 400],\n",
    "     'batch_size': [1, 2, 3, 4, 5, 6],\n",
    "     'epochs': [100, 200],\n",
    "     'optimizer': [Adam, RMSprop],\n",
    "     'losses': [mean_squared_error]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperio as hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 419884.03125\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Hyperio' object has no attribute '_y_range'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-e814175ca83f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mexperiment_no\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgalaxy_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           grid_downsample=0.01)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperio/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, params, dataset_name, experiment_no, model, val_split, shuffle, search_method, reduction_method, reduction_interval, reduction_window, grid_downsample, hyperio_log_name, debug)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_log\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_null\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hyper_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# get the results ready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperio/main.py\u001b[0m in \u001b[0;36m_hyper_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0m_hr_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0m_hr_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_round_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_hr_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_write_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_hr_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperio/main.py\u001b[0m in \u001b[0;36m_run_write_log\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0m_wt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0m_wt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_tostr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_wt_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_wt_out\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/hyperio/main.py\u001b[0m in \u001b[0;36m_dict_tostr\u001b[0;34m(self, d)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m','\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_round_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m','\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m','\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m','\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m','\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Hyperio' object has no attribute '_y_range'"
     ]
    }
   ],
   "source": [
    "h = hy.Hyperio(X_train, y_train,\n",
    "          params=p,\n",
    "          dataset_name='first_test',\n",
    "          experiment_no='1',\n",
    "          model=galaxy_model,\n",
    "          grid_downsample=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 419884.03125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00145: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 419884.03125\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 419884.03125\n",
      "The model needs to have Return in format \"return history, model\"\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable '_hr_out' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-9471974bc236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mexperiment_no\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgalaxy_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           grid_downsample=0.5)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/talos/scan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, params, dataset_name, experiment_no, model, val_split, shuffle, search_method, save_best_model, reduction_method, reduction_interval, reduction_window, grid_downsample, reduction_metric, talos_log_name, debug)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_log\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_null\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_todf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/talos/scan.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The model needs to have Return in format \"return history, model\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_entropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_hr_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0m_hr_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_round_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_hr_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable '_hr_out' referenced before assignment"
     ]
    }
   ],
   "source": [
    "hs = ta.Scan(X_train, y_train,\n",
    "          params=p,\n",
    "          dataset_name='second_test',\n",
    "          experiment_no='2',\n",
    "          model=galaxy_model,\n",
    "          grid_downsample=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
